{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sG8kNaTL--NS"
      },
      "outputs": [],
      "source": [
        "#1)\n",
        "Fundamental idea: combine multiple base learners to produce a single stronger model (reduce variance, bias, or\n",
        "improve predictions) by aggregating diverse hypotheses.\n",
        "Bagging (Bootstrap Aggregating): trains base learners independently on bootstrap samples and aggregates (e.g.,\n",
        "voting/averaging). Objective: reduce variance and prevent overfitting. Works well with high-variance models (e.g.,\n",
        "decision trees).\n",
        "Boosting: trains learners sequentially, where each new learner focuses on correcting errors of previous ones by\n",
        "reweighting samples (or gradients). Objective: reduce bias and build a strong learner from weak learners (e.g.,\n",
        "AdaBoost, Gradient Boosting).\n",
        "\n",
        "#2)\n",
        "RF uses many decorrelated trees built on bootstrap samples and at each split considers a random subset of\n",
        "features, averaging their predictions to reduce variance and avoid the single-tree overfit.\n",
        "Key hyperparameters:\n",
        "1. n_estimators (number of trees): larger reduces variance but increases cost.\n",
        "2. max_features (features considered per split): smaller values increase decorrelation between trees, reducing\n",
        "overfitting; common choices: sqrt(n_features) for classification.\n",
        "\n",
        "#3)\n",
        "Stacking (stacked generalization): trains several base-level models (level-0), then trains a meta-model (level-1) on\n",
        "the base models' out-of-fold predictions to learn how to combine them.\n",
        "Difference from bagging/boosting:\n",
        "Bagging averages independent models; boosting sequentially corrects mistakes.\n",
        "Stacking learns an optimal combination of heterogeneous models using a meta-learner.\n",
        "Example use case: blend logistic regression, random forest, and XGBoost as level-0 models and train a simple\n",
        "meta-model (e.g., logistic regression) to combine their outputs for better classification.\n",
        "\n",
        "#4)\n",
        "OOB (Out-Of-Bag) score: for each tree, samples not included in that tree's bootstrap sample (about ~36% of data)\n",
        "are used as a validation set. Aggregating predictions across trees for their OOB samples yields an OOB estimate.\n",
        "Usefulness: provides an unbiased estimate of generalization performance without needing a separate validation\n",
        "set or cross-validation—convenient for quick model evaluation and hyperparameter tuning.\n",
        "\n",
        "#5)\n",
        "AdaBoost: reweights training samples—misclassified samples receive higher weight; weak learners focus on hard\n",
        "examples.\n",
        "Gradient Boosting: fits each new learner to the negative gradient (residuals) of the loss function — a functional\n",
        "gradient descent viewpoint.\n",
        "Weight adjustment:\n",
        "AdaBoost: explicit sample-weight updates based on previous learners' errors.\n",
        "Gradient Boosting: no explicit sample weights; uses residuals/gradients to guide the next learner.\n",
        "Typical use cases:\n",
        "AdaBoost: simpler boosting for binary classification problems; works with decision stumps.\n",
        "Gradient Boosting (e.g., XGBoost, LightGBM, CatBoost): powerful for tabular data, regression & classification;\n",
        "often wins ML competitions.\n",
        "\n",
        "#6)\n",
        "CatBoost uses target-based statistics (ordered target encoding / permutations) and combinations of categorical\n",
        "features internally to avoid target leakage and overfitting.\n",
        "It applies efficient encoding and symmetric tree construction that naturally handles categorical variables without\n",
        "extensive one-hot encoding or preprocessing, reducing overfitting and runtime memory.\n",
        "\n",
        "#7)\n",
        "1. Load dataset: sklearn.datasets.load_wine()\n",
        "2. Split: train_test_split(test_size=0.30, random_state=42, stratify=y)\n",
        "3. Train KNN (k=5) without scaling; evaluate accuracy + classification_report.\n",
        "4. Apply StandardScaler on features; retrain KNN; compare metrics.\n",
        "5. GridSearchCV over n_neighbors=1..20 and metric in ['minkowski' (p=2 Euclidean), 'manhattan'] (use sklearn's 'p'\n",
        "parameter or specify metrics).\n",
        "6. Train optimized KNN and compare.\n",
        "Expected concise findings (typical outcomes):\n",
        "Unscaled KNN often performs worse because KNN is distance-based and sensitive to feature scales.\n",
        "After StandardScaler, accuracy and F1 usually improve.\n",
        "Grid search typically picks a small-to-moderate k (e.g., 3–9) and often Euclidean distance; optimized model usually\n",
        "beats unscaled baseline.\n",
        "Concise example code snippet (summarized):\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42,stratify=y)\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train,y_train)\n",
        "print(classification_report(y_test, knn.predict(X_test)))\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "Xtr_s = scaler.transform(X_train); Xte_s = scaler.transform(X_test)\n",
        "knn_s = KNeighborsClassifier(n_neighbors=5).fit(Xtr_s,y_train)\n",
        "print(classification_report(y_test, knn_s.predict(Xte_s)))\n",
        "params = {'n_neighbors': list(range(1,21)), 'metric': ['minkowski','manhattan'], 'p':[2,1]}\n",
        "gs = GridSearchCV(KNeighborsClassifier(), params, cv=5).fit(Xtr_s, y_train)\n",
        "best = gs.best_estimator_\n",
        "print(gs.best_params_, accuracy_score(y_test, best.predict(Xte_s)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aa0uUdKAB6qV",
        "outputId": "53aff71f-c64e-47a0-8b8c-aa04b76385b5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.89      0.89        18\n",
            "           1       0.78      0.67      0.72        21\n",
            "           2       0.50      0.60      0.55        15\n",
            "\n",
            "    accuracy                           0.72        54\n",
            "   macro avg       0.72      0.72      0.72        54\n",
            "weighted avg       0.74      0.72      0.73        54\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        18\n",
            "           1       1.00      0.86      0.92        21\n",
            "           2       0.83      1.00      0.91        15\n",
            "\n",
            "    accuracy                           0.94        54\n",
            "   macro avg       0.94      0.95      0.94        54\n",
            "weighted avg       0.95      0.94      0.94        54\n",
            "\n",
            "{'metric': 'minkowski', 'n_neighbors': 4, 'p': 1} 0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8)\n",
        "1. Load breast cancer dataset.\n",
        "2. Standardize features.\n",
        "3. Apply PCA, compute explained_variance_ratio_ and plot scree plot.\n",
        "4. Choose number of components to retain 95% variance (use cumulative sum).\n",
        "5. Transform data to reduced dimensions.\n",
        "6. Train KNN on original standardized data and on PCA-transformed data; compare accuracy.\n",
        "7. Plot first two principal components scatter colored by class.\n",
        "Expected concise findings:\n",
        "Scree plot shows first few components explain most variance (often 2–5 components dominant).\n",
        "Keeping 95% variance typically reduces dimensionality significantly.\n",
        "KNN on PCA data may have similar or slightly lower accuracy but runs faster and mitigates curse of dimensionality.\n",
        "Scatter plot of PC1 vs PC2 often shows class separation for breast cancer dataset.\n",
        "Concise example code snippet (summarized):\n",
        "```\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# load, scale, PCA, keep components for 95% cumulative variance, train KNN and compare.\n",
        "\n",
        "#9)\n",
        "1. make_regression(n_samples=500, n_features=10, noise=0.1, random_state=42)\n",
        "2. Train KNNRegressor with k=5 for Euclidean (p=2) and Manhattan (p=1); compare MSE.\n",
        "3. Evaluate for k in [1,5,10,20,50] and plot k vs MSE to observe bias-variance tradeoff.\n",
        "Expected concise findings:\n",
        "Manhattan vs Euclidean: differences depend on data distribution; Euclidean often slightly better for Gaussian-like\n",
        "features.\n",
        "K small (1) yields low bias, high variance (low train MSE, high test MSE). Large K increases bias, lowers variance.\n",
        "Plot shows U-shaped test MSE vs K, optimal K in middle.\n",
        "Concise example code snippet:\n",
        "```\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "# generate data, split, test different metrics and k values, plot results.\n",
        "\n",
        "#10)\n",
        "1. Load diabetes CSV (provided URL).\n",
        "2. Replace zeros in certain columns with NaN if dataset uses 0 to denote missing (e.g., glucose, BMI), then use\n",
        "KNNImputer to impute.\n",
        "3. Compare KNeighborsClassifier with algorithm='brute', 'kd_tree', 'ball_tree' (note: kd_tree/ball_tree require\n",
        "appropriate metric and numeric data).\n",
        "4. Measure training time (use time.time()) and accuracy on a test split.\n",
        "5. For decision boundary, select two most important features (e.g., via univariate feature importance or model\n",
        "coefficients), train best method and plot 2D decision boundary.\n",
        "Expected concise findings:\n",
        "- KD-Tree and Ball Tree are faster than brute for larger datasets with low-to-moderate dimensions; brute can be\n",
        "competitive for small datasets.\n",
        "- Imputation with KNNImputer helps restore missing values and can improve downstream accuracy.\n",
        "- Decision boundary plot (2 features) visualizes regions of predicted class.\n",
        "Concise example KNNImputer snippet:\n",
        "```\n",
        "from sklearn.impute import KNNImputer\n",
        "import pandas as pd\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/MasteriNeuron/datasets/refs/heads/main/diabetes.csv')"
      ],
      "metadata": {
        "id": "UjVHM8SKCA08"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}